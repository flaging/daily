# 2022年03月11日第三期

## 大模型是AI发展的新一波浪潮
最近国内的AIGC又开始大火起来了,以Diffusion为基础的图像生成模型和ChatGPT/LLAMA文字大模型已经展现出了十分惊人的生成能力了, 但是他们对算力的需求由十分高, 带动了Nvidia/寒武纪等诸多算力提供商的股价上涨, 也有人说,通用人工智能的未来已来.

## 硬件加速

### [Fine-tune FLAN-T5 XL/XXL using DeepSpeed & Hugging Face Transformers](https://www.philschmid.de/fine-tune-flan-t5-deepspeed)
* deepspeed的使用方法实例

### [The Technology Behind BLOOM Training](https://huggingface.co/blog/bloom-megatron-deepspeed)
* 大模型的训练方法,[中文翻译版](https://mp.weixin.qq.com/s/-q9opkoAomd9LZL9phm8bA)
* Deepspeed + Megatron LM
  * ZeRO数据并行
  * Megatron-LM张量并行
  * pipeline并行
  * BF16优化器: **FP16不适合做大模型优化**
  * CUDA融合核函数
  * 训练集打乱重排
  * 嵌入layernorm
  * AliBi位置编码
* 本文内容比较全面,可以作为大模型训练的一个范式

### [万字长文：想训大模型？这里有一份避坑指南](https://hub.baai.ac.cn/view/25052)
* 大模型训练的一些讨论, 仅供参考

### [Serge - LLaMa made easy](https://github.com/nsarrazin/serge)
* 基于llama.cpp的llama接口应用,要求4G内存

### [FidelityFX Super Resolution 2.2 (FSR 2.2)](https://github.com/GPUOpen-Effects/FidelityFX-FSR2)
* AMD的超分应用库

### [Apple Neural Engine (ANE) Transformers](https://github.com/apple/ml-ane-transformers)
* 为apple优化的transformer

## 模型算法

### [What Is ChatGPT Doing … and Why Does It Work?](https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work/)
* 介绍chatGPT的结构和作用, 由于模型并未开源和公开,所以讲的内容都比较基础,可以作为模型算法入门的入门材料.

### [gpt3.5-turbo-pgvector@github](https://github.com/gannonh/gpt3.5-turbo-pgvector)
* 使用GPT建立一个领域知识的应用

### [awesome-ai-coding@gihub](https://github.com/wsxiaoys/awesome-ai-coding)
* AI Code的模型列表

### [Awesome GPT-4@github](https://github.com/radi-cho/awesome-gpt4)
* 一些GPT的论文资料和工具

### [Position Embedding: A Detailed Explanation](https://www.ai-contentlab.com/2023/03/position-embedding-detailed-explanation.html)
* 介绍position embedding的主要结构和功能

### [Machine learning design primer](https://github.com/ibragim-bad/machine-learning-design-primer)
* 深度学习面试资料

### [Stable unCLIP@github](https://github.com/Stability-AI/stablediffusion/blob/main/doc/UNCLIP.MD)
* stable模型的新结构, unCLIP

### [Dolly](https://github.com/databrickslabs/dolly)
* 30分钟即可训练完成的GPT

### [骆驼(Luotuo): Chinese-alpaca-lora](https://github.com/LC1332/Chinese-alpaca-lora)
* 中文大模型,非商汤正式项目
* 相似的还有
  * [Chinese-Vicuna](https://github.com/Facico/Chinese-Vicuna)
  * [Flan-Alpaca: Instruction Tuning from Humans and Machines](https://github.com/declare-lab/flan-alpaca)
  * [Code Alpaca: An Instruction-following LLaMA Model trained on code generation instructions](https://github.com/sahil280114/codealpaca)
  * [alpaca_chinese_dataset](https://github.com/hikariming/alpaca_chinese_dataset)

### [新开源模型裂表@telegram](https://t.me/https1024/11324)
- Google Bard
- Adobe Firefly
- NVIDIA Foundations
- Bing adds new DALL-E
- Opera adds AI
- Microsoft Loop
- Canva puts all the AIs in
- ChatGPT starts adding plugins

## 计算基础

### [聊聊系统设计中的缓存](https://www.codesky.me/archives/cache-design-in-system.wind?continueFlag=89c2c28b4b68da7693b243de88eb3de8)
* 微博@敖天羽对后端缓存设计的一些思考

## 实用工具

### [anandtech](https://www.anandtech.com/)
* 硬件资讯网站.

### [huntly@github](https://github.com/lcomplete/huntly)
* 本地可部署的资讯收集工具,可以收集读过的网页和RSS等

### [hatch@github](https://github.com/pypa/hatch)
* python项目管理工具, 打包配置依赖等, 使用说明见[文档](https://hatch.pypa.io/latest/intro/)

### [wukong-robot@github](https://github.com/wzpan/wukong-robot)
* 接入小爱同学的方式, 没有提供硬件连接的方式

### [ripgrep (rg)@github](https://github.com/BurntSushi/ripgrep)
* grep的替代工具, 使用时命令为**rg**

### [Donut 🍩 : Document Understanding Transformer](https://github.com/clovaai/donut)
* 使用transformer直接做ocr

## 生活杂谈

### [百度AI专家关于文心一言采购AI芯片的一些交流](https://zhuanlan.zhihu.com/p/615090047?utm_medium=social&utm_oi=49336847171584&utm_psn=1622960178810355712&utm_source=ZHShareTargetIDMore)
* 百度的座谈,以pr寒武纪为主, 也有一些有趣的信息
* 推理主要考虑INT8算力,选择互联网/AI独角兽/芯片厂商各一家, 阿里性能好通用性低, 寒武纪,燧原和昆仑芯一般,依图最差
* 训练主要看fp16算力, 昇腾好于燧原和天数
* 寒武纪思元590在某些情况下可以替代A100, 23年采购量约占百度的10%, 但对寒武纪的交付能力存疑
* 百度希望三年国产化率能到50%
* 字节GPU需求10万片


