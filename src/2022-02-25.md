# 2022年2月25日第一期

如果只收集不整理, 等到需要使用的时候也很难找到, 就脱离了收集的本意. 

收纳=收集+整理

## 硬件加速

### [视觉大模型训练和推理加速](视觉大模型训练和推理加速):NVIDIA 在 swin transformer 的训练/推理加速方法

* swin带来了windows attention, 相对于ViT减少了计算量
* 性能分析的主要流程: profiling -> analyze -> optimize
* 训练主要优化点
  * 混合精度矩阵计算: fp16 tensor core
  * 算子融合: 使用apex替换 fusedlayernorm, fusedadam, fMHA
  * torch自定义算子: windows划分处理逻辑自定义算子
* 训练的其他加速方案
  * cuda graph
  * multi stream
  * pure fp16
  * 隐藏通信耗时
* 推理主要优化点
  * 使用cublas/cudnn/cutlass优化矩阵乘
  * 算子融合: [总结图片](https://s6.51cto.com/oss/202301/05/c46291e585c8188e8a0846b01043ea85d04cdf.jpg)
    * 方式1: conv/gemm + elementwise形式直接融合: cublas/cutlass有实现
    * 方式2: conv/gemm + bias + layernorm: conv/gemm单独, bias和后面手动融合
    * fMHA kernel, QKV gemm + bias kernel
  * 矩阵乘法padding: alignment=8提升性能
  * half2/char4格式
    * 提升 memory 的带宽利用效率并降低访存指令数
    * 专有高数学指令降低 kernel 的 latency
    * half2可以1线程处理2数据, 减少空闲线程,降低latency
  * 预先开辟寄存器数组
  * int8量化+QAT/PTQ
    * cublasLt int8矩阵乘法
    * cublasLt支持两种数据布局: IMMA-specific+NT Gemm性能更好, 但需做更多改动; 列优先兼容性较好
  

## 计算基础

## 实用工具

## 生活杂谈
